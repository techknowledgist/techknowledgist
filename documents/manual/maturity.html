<html>

<head>
<meta charset="UTF-8">
<title>Maturity Scores</title>
<link rel="stylesheet" href="main.css" type="text/css" />
</head>

<body>

<h1>Ontology Creation - Maturity Scores</h1>


<p class="navigation">
<a href="index.html">index</a> > <a href="maturity.html">maturity scores</a>
</p>

<p>NOTE: MASSIVE UPDATING NEEDED</p>

<p class="abstract">Maturity scores indicate the maturity of a technology. They
are calculated with a (i) pattern matcher that picks up textual features in the
context of the technology and (ii) some simple statistical calculations over the
results of the matcher. The scores are limited to those terms that occur
frequently enough.</p>

<p>
[ <a href="#input">input</a> 
| <a href="#howto1">pattern matcher</a> 
| <a href="#howto2">term frequencies</a> 
| <a href="#howto3">score generation</a> 
| <a href="#output">output</a> ]
</p>


<a name="input"></a>
<h2>Input description</h2>

<p>The input to this component is partially the same as the input to the
technology classifier, that is, it takes feature files where lines have the
following format (all fields are tab-separated):</p>

<pre class="example">
term_id year term feature+
</pre>

<p>See <a href="classifier.html#input" target="_blank">the classifier input
section</a> of this manual for more details on the input.</p>

<p>In addition to the feature files, maturity score generation also requires the
output of the technology classifier.</p>


<a name="howto1"></a>
<h2>Running the pattern matcher</h2>


<p>The top-level script for running the pattern matcher is</p>

<pre class="example">
ontology/matcher/run_matcher.py
</pre>

<p>The matcher can be run on a corpus or on a file list.</p>


<h3>Running on a corpus</h3>

<p>To run the matcher on all files of a corpus, do the following (new lines and
spaces added for clarity, the command should be on one line):</p>

<pre class="example">
$ python run_matcher.py
    --corpus ../doc_processing/data/patents/corpora/sample-us
    --output maturity
</pre>

<p>This runs the matcher on the corpus specified, which is the example corpus
included in the distribution, and puts the results in a
directory <code>data/o2_matcher/maturity</code> inside the corpus. There is
a <code>--patterns</code> option with possible values <code>MATURITY</code>
and <code>PROMISE</code>, but it does not need be used here since the value
defaults to the former. Using the <code>--verbose</code> option will print
progress on individual files.</p>


<h3>Running on a list of files</h3>

<p>Instead of a corpus, the input to the matcher can be a list of files:</p>

<pre class="example">
$ python run_matcher.py
    --filelist ../classifier/lists/list-sample-us.txt
    --output maturity
</pre>

<p>The files have to be feature
files. The <a href="../../ontology/classifier/lists/list-sample-us.txt"
target="_blank">sample file list</a> used here is that same as used for
the <a href="classifier.html#howto" target="_blank">technology classification
example</a> and refers to the feature files in the <code>sample-us</code>
example corpus.


<a name="howto2"></a>
<h2>Term frequencies</h2>

<p>Maturity scores cannot be realiably calculated for terms with very low
frequencies. Therefore, we use a frequency list as a filter when generating
maturity scores. It was shown in the <a href="classifier.html#output"
target="_blank"> output description section</a> of the classifier manual that
the file <code>classify.MaxEnt.out.s3.scores.sum</code> contains the term
frequencies over a corpus or list of files. However, for maturity score time
series we want to calculate over a series of corpora (or file lists), one for
each year. The merged frequencies can be calculated with
<code>merge_classifier_results.py</code>
in <code>ontology/classifier</code>. This script is used as follows:</p>

<pre class="example">
$ python merge_classifier_results.py OUTPUT_DIRECTORY CLASSIFIER_RESULT+
</pre>

<p>CLASSIFIER_RESULT is a summed classifier result, typically, but not
necessarily, a file with the base
name <code>classify.MaxEnt.out.s3.scores.sum</code>, but it can also be an
expression with unix wild cards. Here is an example where the scores in two
classification results are merged (split over several lines for convenience):

<pre class="example">
$ python merge_classifier_results.py 
    data/merged-2000-2001 
    classifications/2001/classify.MaxEnt.out.s3.scores.sum
    classifications/2002/classify.MaxEnt.out.s3.scores.sum 
</pre>

<p>Using wild cards this would be:</p>

<pre class="example">
    data/merged-2000-2001 
    classifications/????/classify.MaxEnt.out.s3.scores.sum
</pre>

<p>Of course, if there were to be subdirectories for other years with
classification then those would be included as well.</p>

<p>The result are written to <code>data/merged-2000-2001</code>, which has the
following files:</p>

<pre class="example">
merged_term_frequencies.0005.txt
merged_term_frequencies.0010.txt
merged_term_frequencies.0025.txt
merged_term_frequencies.0050.txt
merged_term_frequencies.0100.txt
merged_term_frequencies.all.txt
merged_term_frequencies.info.txt
</pre>

<p>Each (non-info) file has two fields: a total count and the term. One file has
all terms and their frequencies, whereas the others only have the terms that
occur N times, where N is given in the file name. An info file is written with
the version of the code used and all the names of the files that were merged.</p>



<a name="howto3"></a>
<h2>Generating the scores</h2>

The next step is to calculate usage rates from the results of the pattern
matcher. The usage rate is a number between 0 and 1 that is calculated as
follows:

<pre class="example">
log⁡(match-count+1)/(log⁡(highest-match-count+1))
</pre>

Most terms have zero scores, which corresponds to no matches found (that is, no
evidence of usage found). The closer the number gets to 1, the closer the term
usage is relative to the term with the most matches.  To calculate usage rates
we use the script in

<pre class="example">
ontology/maturity/collect_usage_data.py 
</pre>

which combines the match results and the results of the technology classifier,
where the technology classifier, apart from the technology scores, also provides
frequency data needed for calculating the usage rate. Assuming matcher results
and technology scores as created with the example commands above, we create the
usage scores as follows:

<pre class="example">
python collect_usage_data.py \
   --corpus ../doc_processing/data/sample-us \
   --tscores ../classifier/data/classifications/sample-us \
   --output data/usage-sample-us.txt \
   --language en
</pre>

In the output, terms are presented with four numbers as follows:

<pre class="example">
0.0029   0.0000   2   0   accelerometer senses
1.0000   0.0000   1   0   accelerometer sensor
0.0555   0.1201   5   2   accelerometer signal
0.9189   0.0757   1   1   accelerometer signal accel
0.9656   0.0757   1   1   accelerometer signal trace
</pre>

The first column has the average technology score for the term in the corpus,
the second the usage rate, the third the number of documents the term occurs in,
and the fourth the number of matches for the term.

<p>The last step is creating the time series. This is slightly more involved as
with the technology scores, where we could simply take a set of classification
results, each for a particular year, and grab the scores for that year. Maturity
scores are different in several ways:

<ol class="spacy">

<li>Maturity scores tend to be meaningless below a certain term frequency
threshold, we stipulate that a term has to occur in at least 25 documents, but
this is a bit arbitrary.

<li>Maturity scores do not rise and fall like the technology scores can, and
sometimes do. Rather, we assume that maturity scores do not fall in lockstep
with the usage rates and that there is a delay.

<li>Usage scores do not tend to carry great meaning for terms that are not
technologies. For example, the term "caution" can have a high maturity score
because it is often found on the context “use caution”, which triggers one of
the usage patterns.
</ol>

When creating the maturity score time series, we enforce the first two, but not
the last. Now, let’s assume we have four files with usage scores, from the years
2010-2013:

<pre class="example"> 
usage-2010.txt
usage-2011.txt
usage-2012.txt
usage-2013.txt
</pre>

We can create a file with terms with a document frequency of 25 or higher with
the count_terms.py script (also in ontology/maturity):

<pre class="example">
python count_terms.py usage-201*.txt
</pre>

This script will write its output to terms-0025.txt. This is a memory intensive
step. With a few dozen corpora, each with 20-50K files, the machine running this
likely needs 16GB of memory.

<p>Finally, the create_time_series.py script takes the usage files and the file
with frequent terms and creates the time series:</p>

<pre class="example">
python create_time_series.py –t terms-0025.txt usage-201*.txt
</pre>

The script creates two output files, one with a maturity time series based on
usage rates and one with maturity time series based on raw frequencies. The
first of these should be much more useful, the second file is created because an
early version of the system did not generate enough matches to create scores
based on usage rates. The names of the output files are hard-coded and include a
timestamp, for example:

<pre class="example">
out-20141120:195652-frequency-based.txt
out-20141120:195652-usage-based.txt 
</pre>

The first file has scores as given in FUSE phase 1, where each term-year pair
got a 0, 1, or 2 (unavailable, immature, mature). These scores are based on
frequency counts only and are a fallback. The second file has a score between 0
and 1 for each term-year. These scores are based on the usage rates. The content
of the second file looks as follows:

<pre class="example">
0.0228	0.0182	0.0146	0.0117	abrasive fluid
0.0179	0.0143	0.0115	0.0092	abrasive fluids
0.0572	0.0458	0.0706	0.0565	abrasive force
0.0748	0.0598	0.0479	0.0383	abrasive forces
0.1145	0.1155	0.2240	0.2754	abrasive grain
0.3196	0.2934	0.2571	0.2206	abrasive grains
0.1329	0.1063	0.0850	0.0680	abrasive grit
</pre>

The top of the file has a row with headers, which contain the years. These
headers are calculated from the file names, so if the file names do not have
years in them (a sequence of four digits) then the year will not be printed as a
header.



<pre class="example">

</pre>



<a name="output"></a>
<h2>Output description</h2>

<p>The pattern matcher creates six output files:</p>

<pre class="example">
match.info.config.txt
match.info.features.txt
match.info.filelist.txt
match.info.general.txt
match.results.full.txt
match.results.summ.txt
</pre>

<p>The first four contains various pieces of information, including a list of
all features found, a copy of the file list, and a file with the processing
parameters. The file <code>match.results.full.txt</code> contains the raw output
of the matcher:</p>

<pre class="example">
1980    US4192770A.xml_71       maturity-provide        process prev_V=provides
1980    US4192770A.xml_142      maturity-have   boiling point   prev_V=have
1980    US4192770A.xml_158      maturity-activate       alumina prev_V=activated
1980    US4192770A.xml_165      maturity-provide        metallic ions   prev_V=provide
1980    US4192770A.xml_583      maturity-have   affect  prev_V=has
1980    US4192770A.xml_646      maturity-have   effect  prev_V=have
</pre>

<p>The third column has the name of the pattern that matched, the fourth the
term for which we had a match, and the fifth the actual value of the feature
that matched.</p>

<p>Finally, <code>match.results.summ.txt</code> contains a summary with match counts
for each term:</p>

<pre class="example">
   1 member
   1 metallic ions
   2 path
   1 permanent magnet
   1 polarity
   3 poles
   1 positionable control element
</pre>

<p>This file, like the previous file, is tab-separated, but note that there can
be leading spaces (this is for the convenience of the person opening the file and
inspecting its contents).</p>


</body>
</html>
