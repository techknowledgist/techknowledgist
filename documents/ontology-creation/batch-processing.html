<html>

<head>

<title>Batch Processing</title>

<style>

body {
   counter-reset: section;
   counter-reset: subsection;
   width: 800px;
   margin-left: 0.5em;
   font-size: 12pt; }

h2 {
   counter-reset: subsection;
   counter-increment: section; }

h3 {
   counter-increment: subsection; }

h2:before {
   counter(section) ". "; }

h3:before {
   counter(section) "." counter(subsection) ". "; }

p, li {
   font-size: 100%; }

.navigation {
   font-size: 120%; }

.pre {
   font-family: "courier new", monospace; }

p.pre { 
   margin-left: 2em;
   white-space: pre; }

span.pre {
   white-space: normal; }

.indent {
   margin-left: 2em; }

.outdent {
   margin-left: -1.3em; }

.move-up {
   margin-top: -1em; }

.move-up-a-bit {
   margin-top: -0.5em; }

.move-down-a-bit {
   margin-top: 0.5em; }

</style>

</head>

<body>

<p class="navigation">
[ <a href="#pipeline">pipeline</a>
| <a href="#directories">directories</a>
| <a href="#procedure">procedure</a>
] </p>


<a name="pipeline" />

<h2>Configuration file (config/pipeline-ID.txt)</h2>

<p>This file contains the specifications for the pipeline. There is a default version in
config/pipeline-default.txt, but another version can be handed into
step2_document_processing.py with the "--pipeline ID" option, where the default
is 'default'. The pipeline config file is a list of the four processing stages where each
stage is associated with a couple of options. Here is an example:</p>

<p class="pre move-up">
--xml2txt 
--txt2tag
--tag2chk section-filter=off
--df2dfeats version=v0.2
</p>

<p>In general, each processing step is followed by a list of feature-value pairs,
separated by whitespace. Many command line option for step2_document_processing will be
feature-value pairs in this config file. Many more options can/should be added like
chunk-rules, tagger-version, etcetera. Version requirements are coarse-grained and are
only enforced at the tag level (not at the git commit level).</p>

<p>The feature logic is as follows. If a feature like section-filter=off or
section-filter-off is specified, then there has to be an excat match of the configuration
element with the data set. For example, the dataset created by the --tag2chk processing
step would have to exactly match section-filter=off (see section XXX for a description
of how this matching occurs). The one exception is the version feature, which can have a
list a versions (version=v0.1,v0.2); the dataset is allowed to have just one of the list
members.</p>


<p>Open issues:</p>

<ol class="move-up-a-bit">

<li>Need to decide on naming conventions, probably the feature names here should be
either the same as used in calling the python functions in the script (for example,
section_filter_p=True) or the same as the command line options as given to the script (for
example, section-filter-on). In the latter case, we need to allow features that have no
value. The second choice is probably the best.</li>

<li></li>

<li></li>

<li></li>

</ol>



<a name="directories" />

<h2>Directory Structure</h2>

<p>Assuming a directory TARGET_DIR/LANGUAGE, which we will here abbreviate as 'en', we
have two subdirectories, 'config' and 'data'. The 'config' directory contains
configuration settings that pertain to the entire data set. It has the following files:</p>

<p class="pre move-up">
en/config/files.txt
en/config/general.txt
en/config/pipeline-default.txt
en/config/testing-files-0000000-0005000.txt
en/config/training-files-0000000-0005000.txt
</p>

<p>The first contains a list of path pointing to external files, the second has general
configuration settings (sources, target directory, initialization timestamp, etcetera),
the third has the default pipeline, and the fourth and fifth default file sets for
training and testing (these are initialized to the first 500 paths in files.txt). In
'en/data' we have subdirectories that store data created by processing steps (these are
what we call data sets). They are grouped thematically and numbered using prefixes (here
only the ones relevant to document-level processing are listed, there are also directories
with prefix 't' and 'o' for technology tagging and ontology building):</p>

<p class="pre move-up">
en/data/d0_xml
en/data/d1_txt
en/data/d2_seg
en/data/d2_tag
en/data/d3_phr_feats
en/data/d3_phr_occ
en/data/d4_doc_feats
en/data/workspace
</p>

<p>Each of these subdirectories, except for 'workspace', has a further substructure that
is the same for all. Here is the example for the 01 dataset in 'd2_tag':</p>

<p class="pre move-up">
en/data/d2_tag
en/data/d2_tag/01
en/data/d2_tag/01/config
en/data/d2_tag/01/config/pipeline-head.txt
en/data/d2_tag/01/config/pipeline-trace.txt
en/data/d2_tag/01/files
en/data/d2_tag/01/state
en/data/d2_tag/01/state/processed.txt
en/data/d2_tag/01/state/processing-history.txt
</p>

<p>Subdirectories are numbered 01, 02, 03 etcetera, and each number is the identifier of a
dataset, where a dataset consistes of a trace and a head. A data set is defined by a
configuration and state and implements the output of a processing stage for a particular
pipeline configuration. This saves space since different pipelines may share components,
as illustrated below.</p>

<img class="indent" src="dataset-tree.pdf"/>

<p>In a way, these identifiers implement a tree where each path to the leaves corresponds
with a pipeline configuration, in the example above we use a simple partial pipeline with
one two-valued options for each processing step. Some steps inherently do not have any
options (most notable the --populate step which simply copies an external file into the
data area). As mentioned above, config and state define a dataset, we now describe the the
relevant files in the config and state directories. In general, the config director
contains fixed settings whereas the state directory contains things for the data set that
change over time.</p>

<dl class="indent">

  <dt class="pre">config/pipeline-trace.txt</dt>
  <dd>This is the part of the pipeline
    in <span class="pre">en/config/pipeline-X.txt</span> up to, but not including, the
    data set that is created by the current processing step. For example, for --txt2tag
    above it is the list [--populate, --xml2txt shallow=yes|no]. In addition, it will has
    as its first line the path to the pipeline configuration file that this was based
    on</dd>

  <dt class="pre move-down-a-bit">config/pipeline-head.txt</dt>
  <dd>This is the part of the pipeline
    in <span class="pre">en/config/pipeline-X.txt</span> that is the data set that is
    created by the current processing step. For example, for --txt2tag above it is the
    list [--txt2tag tagger-version=x|y]. The image below illustrates how the pipeline file
    point to parts of a full pipeline configuration.
    <img class="xindent" src="head-trace.pdf"/></dd>

  <dt class="pre move-down-a-bit">state/processed.txt</dt>
  <dd>Contains the number of files processed by the processing step, refering to the first
    n lines in <span class="pre">config/files.txt</span>.</dd>

  <dt class="pre move-down-a-bit">state/processing-history.txt</dt>
  <dd>List of records, each record has four fields: files-processed, date/timestamp,
    git-version, and processing time.</dd>
  
</dl>
 
<p>It is not possible to simply use the <span class="pre">config/pipeline-X.txt</span> to
define the configuraiton of a data set since a data set can match more that one pipeline
configuration (pipelines with the same trace and head but with difference after the
head). Also note that for <span class="pre">--xml2txt</span> the trace is always
empty.</p>

<p>The <span class="pre">en/data/d2_tag/01/files</span> directory will have be empty at
initialization but will be filled at processing time with arbitrarily deep sub paths. 

<p>The data set <span class="pre">d0_xml</span> is special in the sence that it contains
copies copies of external files (keeping track of many it has) and that versioning makes
  no sense. So, in all cases, there is only one subdirectory
in <span class="pre">en/data/d0_xml</span>


<a name="procedure"/>

<h2>Procedure</h2>


<h3>Procedure for importing (--populate)</h3>

<p>Input: target directory, language, limit.</p>

<p>Processing steps:</p>

<ol class="move-up">

<li>Create directories and state/processed.txt with content "0\n".</li>

<li>Retrieve number of files processed from state/processed.txt.</li>

<li>Retrieve filenames to be imported, using ontology.utiles.file.get_lines(). This could
  be an emtpy list depending on (1) size of the data set, (2) value in state/processed.txt
  and (3) the value of limit. In this case, should print a warning that no files were
  imported (and the same should happen for later processing steps).</li>

</ol>


<h3>Procedure for non-import steps</h3>

<p>Input: target directory, language, limit, stage identifier, options.</p>


<p>The first step is to find the input data set (here using --txt2tag as an example)</p>

<ol>

<li>Use the stage-to-data mapping, for example txt2tag => { in: <u>txt</u>, out: tag }</li>

<li>Get all data sets D for 'txt'</li>

<li>For all d in D, match d.trace to config.pipeline(txt).trace and d.head to
  config.pipeline(txt)</li>

<li>if one result, return it; if more than one results, exit with warning; if no results,
  warning and exit ("cannot proceed because input requirements are not met")</li>

</ol>


<p>The second step is to find the output data set.</p>

<ol>
  
  <li>Use the stage-to-data mapping, for example txt2tag => { in: txt, out: <u>tag</u> }</li>
  
  <li>Get all data sets D for 'tag'</li>

  <li>For all d<sub>i</sub> in D, match d.trace to config.pipeline(tag).trace and d.head to
    config.pipeline(tag)</li>

  <li>if one result, return it; if more than one, print warning and exit; if no results,
    initialize a new data set:
  
    <ol>
      <li>
	initialize_dataset(i+1, tag, config, processing_stage, output_name)<br/>
	- create target_path/language/tag/i+1/config<br/>
	- create target_path/language/tag/i+1/files<br/>
	where config has processed.txt with content "0\n", pipeline-trace.txt and
	pipeline-head.txt are from config/pipeline-X.txt, and processing-history.txt is
	empty.<br/>
	this is all done with a class named DataSet, which embeds the config (which has
	target_path and language
      </li>
    </ol>
  </li>

</ol>


<p>The third step is to check whether the input data set has enough processed documents to meet th elimit requirement given the processing state on the output data set.</p>

<p class="pre move-up1">
input.processed > output.processed + limit
</p>


<h2>Some random notes and reminders</h2>

<p>Need version of stanford tagger in config?</p>

<p>Need to get rid of absolute path in xml2txt. This is probably just used by Peter and
would be solved with properly setting up the document structure parser as a submodule.</p>

<p>The existence of a data set is no guarantee that the ... (or is it?)</p>

<p></p>

<p></p>

<p></p>



</body>

</html>

