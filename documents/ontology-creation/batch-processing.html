<html>

<head>

<title>Batch Processing</title>

<link rel="stylesheet" href="main.css" type="text/css" />
<link rel="stylesheet" href="batch-processing.css" type="text/css" />

</head>

<body>

<p class="navigation">
[ <a href="#pipeline">pipeline</a>
| <a href="#directories">directories</a>
| <a href="#procedure">procedure</a>
] </p>


<h1>Batch Processing Design</h1>

<p class="author">Marc Verhagen, last updated February 21, 2013</p>

<p>This documents descirbes the general design of the batch processing scripts that start
with "step".</p>


<a name="pipeline" />

<h2>Configuration file (config/pipeline-ID.txt)</h2>

<p>This file contains the specifications for the pipeline. There is a default version in
config/pipeline-default.txt, but another version can be handed into
step2_document_processing.py with the "--pipeline ID" option, where the default is
'default'. Alternative pipeline scripts are expected to be in the <span class="pre">config</span>
directory. The pipeline config file is a list of the four processing stages where each
stage is associated with a couple of options. Here is an example:</p>

<p class="pre move-up">
--xml2txt 
--txt2tag
--tag2chk section-filter=off
--pf2dfeats version=v0.2
</p>

<p>In general, each processing step is followed by a list of feature-value pairs,
separated by whitespace. Many command line options for step2_document_processing will be
feature-value pairs in this config file. Many more options can/should be added like
chunk-rules, tagger-version, etcetera. the version option tells hands requirements on the
git commit to the processing scripts. Version requirements are coarse-grained and are only
enforced at the tag level not at the git commit level. With the expample above, the system
will not require that the commit is exactly the commit with tag=v0.2, rather, it specifies
that the result of git-describe should start with 'v0.2'.</p>

<p>The configuration file specifies a processing chain. A processing chain is associated
with a set of datasets, where each data set corresponds to the output of a processing
step. The pipeline configuration uniquely identifers input and output datasets for each
processing step. The feature logic is as follows. If a feature like section-filter=off or
section-filter-off is specified, then there has to be an exact match of the configuration
element with the data set. If no features are specified, the dataset cannot have a feature
specified. For example, with the config file above, the dataset created by the --tag2chk
processing step would have to exactly match section-filter=off.</p>


<p>Open issues:</p>

<ol class="move-up-a-bit">

<li>Need to decide on naming conventions, probably the feature names here should be
either the same as used in calling the python functions in the script (for example,
section_filter_p=True) or the same as the command line options as given to the script (for
example, section-filter-on). In the latter case, we need to allow features that have no
value. The second choice is probably the best.</li>

<li>Enforcing the processing steps to be limited to the version specified is not yet
implemented.</li>

<li>The one exception on the exact matching for datasets is the version feature, which can
have a list of versions (version=v0.1,v0.2); the dataset is allowed to have just one of the
list members. This is not implemented yet either.</li>

</ol>


<a name="directories" />

<h2>Directory Structure</h2>

<p>Assuming a directory TARGET_DIR/LANGUAGE, which we will here abbreviate as 'en', we
have two subdirectories, 'config' and 'data'. The 'config' directory contains
configuration settings that pertain to the entire data set. It has the following files:</p>

<p class="pre move-up">
en/config/files.txt
en/config/general.txt
en/config/pipeline-default.txt
en/config/testing-files-0000000-0005000.txt
en/config/training-files-0000000-0005000.txt
</p>

<p>The first contains a list of paths pointing to external files, the second has general
configuration settings (sources, target directory, initialization timestamp, etcetera),
the third has the default pipeline, and the fourth and fifth default file sets for
training and testing (these are initialized to the first 500 paths in files.txt). In
<span class="pre">en/data</span> we have subdirectories that store data created by
processing steps (the datasets). They are grouped thematically and
numbered using prefixes (here only the ones relevant to document-level processing are
listed, there are also directories with prefix 't' and 'o' for technology tagging and
ontology building):</p>

<p class="pre move-up">
en/data/d0_xml
en/data/d1_txt
en/data/d2_seg
en/data/d2_tag
en/data/d3_phr_feats
en/data/d3_phr_occ
en/data/d4_doc_feats
en/data/workspace
</p>

<p>For the document processing phase, there is subdirectory for each data type, with data
types for the xml source, document structure representaiton, segmented data (Chinese
only), tagged data, and chunked data with various feature sets. All the subdirectories
listed above, except for <span class="pre">workspace</span>, contains datasets where each
dataset is in a numbered directory that reflects the dataset identifier (which is unique
for each data type, not unique accross datatypes. These have a substructure that is the
same for all. Here is the example for the <span class="pre">d2_tag/01</span>:</p>

<p class="pre move-up">
en/data/d2_tag
en/data/d2_tag/01
en/data/d2_tag/01/config
en/data/d2_tag/01/config/pipeline-head.txt
en/data/d2_tag/01/config/pipeline-trace.txt
en/data/d2_tag/01/files
en/data/d2_tag/01/state
en/data/d2_tag/01/state/processed.txt
en/data/d2_tag/01/state/processing-history.txt
</p>

<p>A data set is defined by a configuration and state and implements the output of a
processing stage for a particular pipeline configuration. A dataset configuration consists
of a trace and a head, where the head contains the options for the processing step
creating the current data type (the datatype of the dataset) and the trace the options for
all previous stages. This design saves space since different pipelines may share
components, as illustrated below. </p>

<img class="indent" src="dataset-tree.jpg" width="700"/>

<p>In a way, these identifiers implement a tree where each path to the leaves corresponds
with a pipeline configuration, in the example above we use a simple partial pipeline with
one two-valued option for each processing step. Some steps inherently do not have any
options (most notable the --populate step which simply copies an external file into the
data area). As mentioned above, config and state define a dataset, we now describe the the
relevant files in the config and state directories. In general, the config director
contains fixed settings whereas the state directory contains things for the data set that
change over time.</p>

<dl class="indent">

  <dt class="pre">config/pipeline-trace.txt</dt>
  <dd>This is the part of the pipeline
    in <span class="pre">en/config/pipeline-X.txt</span> up to, but not including, the
    data set that is created by the current processing step. For example, for --txt2tag
    above it is the list [--populate, --xml2txt shallow=yes|no].</dd>

  <dt class="pre move-down-a-bit">config/pipeline-head.txt</dt>
  <dd>This is the part of the pipeline
    in <span class="pre">en/config/pipeline-X.txt</span> that is the data type that is
    created by the current processing step. For example, for --txt2tag above it is the
    list [--txt2tag tagger-version=x|y]. The image below illustrates how the pipeline file
    point to parts of a full pipeline configuration.
    <img src="head-trace.jpg" width="450"/></dd>

  <dt class="pre move-down-a-bit">state/processed.txt</dt>
  <dd>Contains the number of files processed by the processing step, refering to the first
    n lines in <span class="pre">config/files.txt</span>.</dd>

  <dt class="pre move-down-a-bit">state/processing-history.txt</dt>
  <dd>List of records, each record has four fields: files-processed, date/timestamp,
    git-version, and processing time.<br/>
    TODO: add number of first file processed.</dd>
  
</dl>
 
<p>It is not possible to simply use the <span class="pre">config/pipeline-X.txt</span> to
define the configuration of a data set since a data set can match more that one pipeline
configuration (pipelines with the same trace and head, but with differences after the
head). Also note that for <span class="pre">--xml2txt</span> the trace is always
empty.</p>

<p>The <span class="pre">en/data/d2_tag/01/files</span> directory will be empty at
initialization but will be filled at processing time with arbitrarily deep sub paths. 

<p>The data set <span class="pre">d0_xml</span> is special in the sence that it contains
copies of external files (keeping track of many it has) and that versioning makes no
sense. So, in all cases, there is only one subdirectory
in <span class="pre">en/data/d0_xml</span>.


<a name="procedure"/>

<h2>Procedure</h2>

<p>NOTE: this section is out-of-date.</p>


<h3>Procedure for importing (--populate)</h3>

<p>Input: target directory, language, limit.</p>

<p>Processing steps:</p>

<ol class="move-up">

<li>Create directories and state/processed.txt with content "0\n".</li>

<li>Retrieve number of files processed from state/processed.txt.</li>

<li>Retrieve filenames to be imported, using ontology.utiles.file.get_lines(). This could
  be an emtpy list depending on (1) size of the data set, (2) value in state/processed.txt
  and (3) the value of limit. In this case, should print a warning that no files were
  imported (and the same should happen for later processing steps).</li>

</ol>


<h3>Procedure for non-import steps</h3>

<p>Input: target directory, language, limit, stage identifier, options.</p>


<p>The first step is to find the input data set (here using --txt2tag as an example)</p>

<ol>

<li>Use the stage-to-data mapping, for example txt2tag => { in: <u>txt</u>, out: tag }</li>

<li>Get all data sets D for 'txt'</li>

<li>For all d in D, match d.trace to config.pipeline(txt).trace and d.head to
  config.pipeline(txt)</li>

<li>if one result, return it; if more than one results, exit with warning; if no results,
  warning and exit ("cannot proceed because input requirements are not met")</li>

</ol>


<p>The second step is to find the output data set.</p>

<ol>
  
  <li>Use the stage-to-data mapping, for example txt2tag => { in: txt, out: <u>tag</u> }</li>
  
  <li>Get all data sets D for 'tag'</li>

  <li>For all d<sub>i</sub> in D, match d.trace to config.pipeline(tag).trace and d.head to
    config.pipeline(tag)</li>

  <li>if one result, return it; if more than one, print warning and exit; if no results,
    initialize a new data set:
  
    <ol>
      <li>
	initialize_dataset(i+1, tag, config, processing_stage, output_name)<br/>
	- create target_path/language/tag/i+1/config<br/>
	- create target_path/language/tag/i+1/files<br/>
	where config has processed.txt with content "0\n", pipeline-trace.txt and
	pipeline-head.txt are from config/pipeline-X.txt, and processing-history.txt is
	empty.<br/>
	this is all done with a class named DataSet, which embeds the config (which has
	target_path and language
      </li>
    </ol>
  </li>

</ol>


<p>The third step is to check whether the input data set has enough processed documents to
meet the limit requirement given the processing state on the output data set.</p>

<p class="pre move-up">
input.processed > output.processed + limit
</p>


<h2>Some random notes and reminders</h2>

<p>Need version of stanford tagger in config?</p>

<p>Need to get rid of absolute path in xml2txt. This is probably just used by Peter and
would be solved with properly setting up the document structure parser as a submodule.</p>

<p>The existence of a data set is no guarantee that the ... (or is it?)</p>

<p>Could probably deal with the segmentation for Chinese by adding d2_seg as an output for
txt2tag and dealing with the extra data set for Chinese,</p>

<p>Need to implement the versioning requirements.</p>

<p></p>



</body>

</html>

